{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From <https://github.com/mp2893/retain/blob/master/retain.py>\n",
    "\n",
    "Code written by Edward Choi (mp2893@gatech.edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseDir = \"/work/data/\"\n",
    "generatedDir = baseDir + \"generated/\"\n",
    "fileBase = generatedDir + \"mimic3\"\n",
    "\n",
    "# type=str\n",
    "# The path to the Pickled file containing visit information of patients\n",
    "seqFile= fileBase + '.seqs'\n",
    "\n",
    "# type=int\n",
    "# The number of unique input medical codes\n",
    "inputDimSize=20000\n",
    "\n",
    "# type=str\n",
    "# The path to the Pickled file containing label information of patients\n",
    "labelFile= fileBase + '.morts'\n",
    "\n",
    "# type=int\n",
    "# The number of unique label medical codes\n",
    "numClass=1\n",
    "\n",
    "# The path to the output models. The models will be saved after every epoch\n",
    "outFile= fileBase + '.model'\n",
    "\n",
    "# The path to the Pickled file containing durations between visits of patients. \n",
    "# If you are not using duration information, do not use this option\n",
    "timeFile=''\n",
    "# timeFile = fileBase + '.dates'\n",
    "\n",
    "# type=str\n",
    "# The path to the Numpy-compressed file containing the model parameters. \n",
    "# Use this option if you want to re-train an existing model\n",
    "# modelFile= fileBase + '.model.npz'\n",
    "modelFile= ''\n",
    "\n",
    "# Use logarithm of time duration to dampen the impact of the outliers\n",
    "useLogTime=True\n",
    "\n",
    "# The path to the Pickled file containing the representation vectors of medical codes. \n",
    "# If you are not using medical code representations, do not use this option\n",
    "# embFile='embFile.txt'\n",
    "# Note: NO EMB FILE HERE\n",
    "embFile=''\n",
    "\n",
    "\n",
    "# The size of the visit embedding. \n",
    "# If you are not providing your own medical code vectors, you can specify this value\n",
    "embDimSize=128\n",
    "\n",
    "# If you are using randomly initialized code representations, always use this option. \n",
    "# If you are using an external medical code representations, \n",
    "# and you want to fine-tune them as you train RETAIN, use this option\n",
    "embFineTune=True\n",
    "\n",
    "# The size of the hidden layers of the GRU responsible for generating alpha weights\n",
    "alphaHiddenDimSize=128\n",
    "\n",
    "# The size of the hidden layers of the GRU responsible for generating beta weights\n",
    "betaHiddenDimSize=128\n",
    "\n",
    "# The size of a single mini-batch\n",
    "batchSize=100\n",
    "\n",
    "# type=int\n",
    "# The number of training epochs\n",
    "max_epochs=30\n",
    "\n",
    "# type=float\n",
    "# L2 regularization for the final classifier weight w\n",
    "L2_output=0.001\n",
    "\n",
    "# type=float\n",
    "# L2 regularization for the input embedding weight W_emb\n",
    "L2_emb=0.001\n",
    "\n",
    "# type=float\n",
    "# L2 regularization for the alpha generating weight w_alpha\n",
    "L2_alpha=0.001\n",
    "\n",
    "# type=float\n",
    "# L2 regularization for the input embedding weight W_beta\n",
    "L2_beta=0.001\n",
    "\n",
    "# type=float\n",
    "# Decides how much you want to keep during the dropout between the embedded input and\n",
    "# the alpha & beta generation process\n",
    "keepProbEmb=0.5\n",
    "\n",
    "# type=float\n",
    "# Decides how much you want to keep during the dropout between \n",
    "# the context vector c_i and the final classifier\n",
    "keepProbContext=0.5\n",
    "\n",
    "# type=float\n",
    "# A small value to prevent log(0)\n",
    "logEps=1e-8\n",
    "\n",
    "# type=str\n",
    "# ['adadelta','adam']\n",
    "# Select which solver to train RETAIN: adadelta, or adam.\n",
    "solver='adadelta'\n",
    "\n",
    "# Use an alternative way to load the dataset. Instead of you having to provide a trainign set, \n",
    "# validation set, test set, this will automatically divide the dataset.\n",
    "# simpleLoad=False\n",
    "simpleLoad=False\n",
    "\n",
    "# Print output after every 100 mini-batches\n",
    "verbose=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, random\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "from collections import OrderedDict\n",
    "import argparse\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano import config\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_TEST_RATIO = 0.2\n",
    "_VALIDATION_RATIO = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip(zipped):\n",
    "    new_params = OrderedDict()\n",
    "    for key, value in zipped.iteritems():\n",
    "        new_params[key] = value.get_value()\n",
    "    return new_params\n",
    "\n",
    "def numpy_floatX(data):\n",
    "    return np.asarray(data, dtype=config.floatX)\n",
    "\n",
    "def get_random_weight(dim1, dim2, left=-0.1, right=0.1):\n",
    "    return np.random.uniform(left, right, (dim1, dim2)).astype(config.floatX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(infile):\n",
    "    Wemb = np.array(pickle.load(open(infile, 'rb'))).astype(config.floatX)\n",
    "    return Wemb\n",
    "\n",
    "def padMatrixWithTime(seqs, times, options):\n",
    "    lengths = np.array([len(seq) for seq in seqs]).astype('int32')\n",
    "    n_samples = len(seqs)\n",
    "    maxlen = np.max(lengths)\n",
    "\n",
    "    x = np.zeros((maxlen, n_samples, options['inputDimSize'])).astype(config.floatX)\n",
    "    t = np.zeros((maxlen, n_samples)).astype(config.floatX)\n",
    "    for idx, (seq,time) in enumerate(zip(seqs,times)):\n",
    "        for xvec, subseq in zip(x[:,idx,:], seq):\n",
    "            xvec[subseq] = 1.\n",
    "        t[:lengths[idx], idx] = time\n",
    "\n",
    "    if options['useLogTime']: t = np.log(t + 1.)\n",
    "\n",
    "    return x, t, lengths\n",
    "\n",
    "def padMatrixWithoutTime(seqs, options):\n",
    "    lengths = np.array([len(seq) for seq in seqs]).astype('int32')\n",
    "    n_samples = len(seqs)\n",
    "    maxlen = np.max(lengths)\n",
    "\n",
    "    x = np.zeros((maxlen, n_samples, options['inputDimSize'])).astype(config.floatX)\n",
    "    for idx, seq in enumerate(seqs):\n",
    "        for xvec, subseq in zip(x[:,idx,:], seq):\n",
    "            xvec[subseq] = 1.\n",
    "\n",
    "    return x, lengths\n",
    "\n",
    "def load_data_simple(seqFile, labelFile, timeFile=''):\n",
    "    sequences = np.array(pickle.load(open(seqFile, 'rb')))\n",
    "    labels = np.array(pickle.load(open(labelFile, 'rb')))\n",
    "    if len(timeFile) > 0:\n",
    "        times = np.array(pickle.load(open(timeFile, 'rb')))\n",
    "\n",
    "    dataSize = len(labels)\n",
    "    np.random.seed(0)\n",
    "    ind = np.random.permutation(dataSize)\n",
    "    nTest = int(_TEST_RATIO * dataSize)\n",
    "    nValid = int(_VALIDATION_RATIO * dataSize)\n",
    "\n",
    "    test_indices = ind[:nTest]\n",
    "    valid_indices = ind[nTest:nTest+nValid]\n",
    "    train_indices = ind[nTest+nValid:]\n",
    "\n",
    "    train_set_x = sequences[train_indices]\n",
    "    train_set_y = labels[train_indices]\n",
    "    test_set_x = sequences[test_indices]\n",
    "    test_set_y = labels[test_indices]\n",
    "    valid_set_x = sequences[valid_indices]\n",
    "    valid_set_y = labels[valid_indices]\n",
    "    train_set_t = None\n",
    "    test_set_t = None\n",
    "    valid_set_t = None\n",
    "\n",
    "    if len(timeFile) > 0:\n",
    "        train_set_t = times[train_indices]\n",
    "        test_set_t = times[test_indices]\n",
    "        valid_set_t = times[valid_indices]\n",
    "\n",
    "    def len_argsort(seq):\n",
    "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "\n",
    "    train_sorted_index = len_argsort(train_set_x)\n",
    "    train_set_x = [train_set_x[i] for i in train_sorted_index]\n",
    "    train_set_y = [train_set_y[i] for i in train_sorted_index]\n",
    "\n",
    "    valid_sorted_index = len_argsort(valid_set_x)\n",
    "    valid_set_x = [valid_set_x[i] for i in valid_sorted_index]\n",
    "    valid_set_y = [valid_set_y[i] for i in valid_sorted_index]\n",
    "\n",
    "    test_sorted_index = len_argsort(test_set_x)\n",
    "    test_set_x = [test_set_x[i] for i in test_sorted_index]\n",
    "    test_set_y = [test_set_y[i] for i in test_sorted_index]\n",
    "\n",
    "    if len(timeFile) > 0:\n",
    "        train_set_t = [train_set_t[i] for i in train_sorted_index]\n",
    "        valid_set_t = [valid_set_t[i] for i in valid_sorted_index]\n",
    "        test_set_t = [test_set_t[i] for i in test_sorted_index]\n",
    "\n",
    "    train_set = (train_set_x, train_set_y, train_set_t)\n",
    "    valid_set = (valid_set_x, valid_set_y, valid_set_t)\n",
    "    test_set = (test_set_x, test_set_y, test_set_t)\n",
    "\n",
    "    return train_set, valid_set, test_set\n",
    "\n",
    "\n",
    "def load_data(seqFile, labelFile, timeFile):\n",
    "    train_set_x = pickle.load(open(seqFile+'.train', 'rb'))\n",
    "    valid_set_x = pickle.load(open(seqFile+'.valid', 'rb'))\n",
    "    test_set_x = pickle.load(open(seqFile+'.test', 'rb'))\n",
    "    train_set_y = pickle.load(open(labelFile+'.train', 'rb'))\n",
    "    valid_set_y = pickle.load(open(labelFile+'.valid', 'rb'))\n",
    "    test_set_y = pickle.load(open(labelFile+'.test', 'rb'))\n",
    "    train_set_t = None\n",
    "    valid_set_t = None\n",
    "    test_set_t = None\n",
    "\n",
    "    if len(timeFile) > 0:\n",
    "        train_set_t = pickle.load(open(timeFile+'.train', 'rb'))\n",
    "        valid_set_t = pickle.load(open(timeFile+'.valid', 'rb'))\n",
    "        test_set_t = pickle.load(open(timeFile+'.test', 'rb'))\n",
    "\n",
    "    def len_argsort(seq):\n",
    "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "\n",
    "    train_sorted_index = len_argsort(train_set_x)\n",
    "    train_set_x = [train_set_x[i] for i in train_sorted_index]\n",
    "    train_set_y = [train_set_y[i] for i in train_sorted_index]\n",
    "\n",
    "    valid_sorted_index = len_argsort(valid_set_x)\n",
    "    valid_set_x = [valid_set_x[i] for i in valid_sorted_index]\n",
    "    valid_set_y = [valid_set_y[i] for i in valid_sorted_index]\n",
    "\n",
    "    test_sorted_index = len_argsort(test_set_x)\n",
    "    test_set_x = [test_set_x[i] for i in test_sorted_index]\n",
    "    test_set_y = [test_set_y[i] for i in test_sorted_index]\n",
    "\n",
    "    if len(timeFile) > 0:\n",
    "        train_set_t = [train_set_t[i] for i in train_sorted_index]\n",
    "        valid_set_t = [valid_set_t[i] for i in valid_sorted_index]\n",
    "        test_set_t = [test_set_t[i] for i in test_sorted_index]\n",
    "\n",
    "    train_set = (train_set_x, train_set_y, train_set_t)\n",
    "    valid_set = (valid_set_x, valid_set_y, valid_set_t)\n",
    "    test_set = (test_set_x, test_set_y, test_set_t)\n",
    "\n",
    "    return train_set, valid_set, test_set\n",
    "\n",
    "def calculate_auc(test_model, dataset, options):\n",
    "    batchSize = options['batchSize']\n",
    "    useTime = options['useTime']\n",
    "\n",
    "    n_batches = int(np.ceil(float(len(dataset[0])) / float(batchSize)))\n",
    "    scoreVec = []\n",
    "    for index in xrange(n_batches):\n",
    "        batchX = dataset[0][index*batchSize:(index+1)*batchSize]\n",
    "        if useTime:\n",
    "            batchT = dataset[2][index*batchSize:(index+1)*batchSize]\n",
    "            x, t, lengths = padMatrixWithTime(batchX, batchT, options)\n",
    "            scores = test_model(x, t, lengths)\n",
    "        else:\n",
    "            x, lengths = padMatrixWithoutTime(batchX, options)\n",
    "            scores = test_model(x, lengths)\n",
    "        scoreVec.extend(list(scores))\n",
    "    labels = dataset[1]\n",
    "    auc = roc_auc_score(list(labels), list(scoreVec))\n",
    "    return auc\n",
    "\n",
    "def calculate_cost(test_model, dataset, options):\n",
    "    batchSize = options['batchSize']\n",
    "    useTime = options['useTime']\n",
    "\n",
    "    costSum = 0.0\n",
    "    dataCount = 0\n",
    "\n",
    "    n_batches = int(np.ceil(float(len(dataset[0])) / float(batchSize)))\n",
    "    for index in xrange(n_batches):\n",
    "        batchX = dataset[0][index*batchSize:(index+1)*batchSize]\n",
    "        if useTime:\n",
    "            batchT = dataset[2][index*batchSize:(index+1)*batchSize]\n",
    "            x, t, lengths = padMatrixWithTime(batchX, batchT, options)\n",
    "            y = np.array(dataset[1][index*batchSize:(index+1)*batchSize]).astype(config.floatX)\n",
    "            scores = test_model(x, y, t, lengths)\n",
    "        else:\n",
    "            x, lengths = padMatrixWithoutTime(batchX, options)\n",
    "            y = np.array(dataset[1][index*batchSize:(index+1)*batchSize]).astype(config.floatX)\n",
    "            scores = test_model(x, y, lengths)\n",
    "        costSum += scores * len(batchX)\n",
    "        dataCount += len(batchX)\n",
    "    return costSum / dataCount\n",
    "\n",
    "def print2file(buf, outFile):\n",
    "    outfd = open(outFile, 'a')\n",
    "    outfd.write(buf + '\\n')\n",
    "    outfd.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(options):\n",
    "    params = OrderedDict()\n",
    "    timeFile = options['timeFile']\n",
    "    embFile = options['embFile']\n",
    "    embDimSize = options['embDimSize']\n",
    "    inputDimSize = options['inputDimSize']\n",
    "    alphaHiddenDimSize= options['alphaHiddenDimSize']\n",
    "    betaHiddenDimSize= options['betaHiddenDimSize']\n",
    "    numClass = options['numClass']\n",
    "\n",
    "    if len(embFile) > 0: \n",
    "        print 'using external code embedding'\n",
    "        params['W_emb'] = load_embedding(embFile)\n",
    "        embDimSize = params['W_emb'].shape[1]\n",
    "    else: \n",
    "        print 'using randomly initialized code embedding'\n",
    "        params['W_emb'] = get_random_weight(inputDimSize, embDimSize)\n",
    "\n",
    "    gruInputDimSize = embDimSize\n",
    "    if len(timeFile) > 0: gruInputDimSize = embDimSize + 1\n",
    "\n",
    "    params['W_gru_a'] = get_random_weight(gruInputDimSize, 3*alphaHiddenDimSize)\n",
    "    params['U_gru_a'] = get_random_weight(alphaHiddenDimSize, 3*alphaHiddenDimSize)\n",
    "    params['b_gru_a'] = np.zeros(3 * alphaHiddenDimSize).astype(config.floatX)\n",
    "\n",
    "    params['W_gru_b'] = get_random_weight(gruInputDimSize, 3*betaHiddenDimSize)\n",
    "    params['U_gru_b'] = get_random_weight(betaHiddenDimSize, 3*betaHiddenDimSize)\n",
    "    params['b_gru_b'] = np.zeros(3 * betaHiddenDimSize).astype(config.floatX)\n",
    "\n",
    "    params['w_alpha'] = get_random_weight(alphaHiddenDimSize, 1)\n",
    "    params['b_alpha'] = np.zeros(1).astype(config.floatX)\n",
    "    params['W_beta'] = get_random_weight(betaHiddenDimSize, embDimSize)\n",
    "    params['b_beta'] = np.zeros(embDimSize).astype(config.floatX)\n",
    "    params['w_output'] = get_random_weight(embDimSize, numClass)\n",
    "    params['b_output'] = np.zeros(numClass).astype(config.floatX)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_params(options):\n",
    "    return np.load(options['modelFile'])\n",
    "\n",
    "def init_tparams(params, options):\n",
    "    tparams = OrderedDict()\n",
    "    for key, value in params.iteritems():\n",
    "        if not options['embFineTune'] and key == 'W_emb': continue\n",
    "        tparams[key] = theano.shared(value, name=key)\n",
    "    return tparams\n",
    "\n",
    "def dropout_layer(state_before, use_noise, trng, keep_prob=0.5):\n",
    "    proj = T.switch(\n",
    "        use_noise,\n",
    "        state_before * trng.binomial(state_before.shape, p=keep_prob, n=1, dtype=state_before.dtype) / keep_prob,\n",
    "        state_before)\n",
    "    return proj\n",
    "\n",
    "def _slice(_x, n, dim):\n",
    "    if _x.ndim == 3:\n",
    "        return _x[:, :, n*dim:(n+1)*dim]\n",
    "    return _x[:, n*dim:(n+1)*dim]\n",
    "\n",
    "def gru_layer(tparams, emb, name, hiddenDimSize):\n",
    "    timesteps = emb.shape[0]\n",
    "    if emb.ndim == 3: n_samples = emb.shape[1]\n",
    "    else: n_samples = 1\n",
    "\n",
    "    def stepFn(wx, h, U_gru):\n",
    "        uh = T.dot(h, U_gru)\n",
    "        r = T.nnet.sigmoid(_slice(wx, 0, hiddenDimSize) + _slice(uh, 0, hiddenDimSize))\n",
    "        z = T.nnet.sigmoid(_slice(wx, 1, hiddenDimSize) + _slice(uh, 1, hiddenDimSize))\n",
    "        h_tilde = T.tanh(_slice(wx, 2, hiddenDimSize) + r * _slice(uh, 2, hiddenDimSize))\n",
    "        h_new = z * h + ((1. - z) * h_tilde)\n",
    "        return h_new\n",
    "\n",
    "    Wx = T.dot(emb, tparams['W_gru_'+name]) + tparams['b_gru_'+name]\n",
    "    results, updates = theano.scan(fn=stepFn, sequences=[Wx], outputs_info=T.alloc(numpy_floatX(0.0), n_samples, hiddenDimSize), non_sequences=[tparams['U_gru_'+name]], name='gru_layer', n_steps=timesteps)\n",
    "\n",
    "    return results\n",
    "\n",
    "def adadelta(tparams, grads, x, y, lengths, cost, options, t=None):\n",
    "    zipped_grads = [theano.shared(p.get_value() * numpy_floatX(0.), name='%s_grad' % k) for k, p in tparams.iteritems()]\n",
    "    running_up2 = [theano.shared(p.get_value() * numpy_floatX(0.), name='%s_rup2' % k) for k, p in tparams.iteritems()]\n",
    "    running_grads2 = [theano.shared(p.get_value() * numpy_floatX(0.), name='%s_rgrad2' % k) for k, p in tparams.iteritems()]\n",
    "\n",
    "    zgup = [(zg, g) for zg, g in zip(zipped_grads, grads)]\n",
    "    rg2up = [(rg2, 0.95 * rg2 + 0.05 * (g ** 2)) for rg2, g in zip(running_grads2, grads)]\n",
    "\n",
    "    if len(options['timeFile']) > 0:\n",
    "        f_grad_shared = theano.function([x, y, t, lengths], cost, updates=zgup + rg2up, name='adadelta_f_grad_shared')\n",
    "    else:\n",
    "        f_grad_shared = theano.function([x, y, lengths], cost, updates=zgup + rg2up, name='adadelta_f_grad_shared')\n",
    "\n",
    "    updir = [-T.sqrt(ru2 + 1e-6) / T.sqrt(rg2 + 1e-6) * zg for zg, ru2, rg2 in zip(zipped_grads, running_up2, running_grads2)]\n",
    "    ru2up = [(ru2, 0.95 * ru2 + 0.05 * (ud ** 2)) for ru2, ud in zip(running_up2, updir)]\n",
    "    param_up = [(p, p + ud) for p, ud in zip(tparams.values(), updir)]\n",
    "\n",
    "    f_update = theano.function([], [], updates=ru2up + param_up, on_unused_input='ignore', name='adadelta_f_update')\n",
    "\n",
    "    return f_grad_shared, f_update\n",
    "\n",
    "def adam(cost, tparams, lr=0.0002, b1=0.1, b2=0.001, e=1e-8):\n",
    "    updates = []\n",
    "    grads = T.grad(cost, wrt=tparams.values())\n",
    "    i = theano.shared(numpy_floatX(0.))\n",
    "    i_t = i + 1.\n",
    "    fix1 = 1. - (1. - b1)**i_t\n",
    "    fix2 = 1. - (1. - b2)**i_t\n",
    "    lr_t = lr * (T.sqrt(fix2) / fix1)\n",
    "    for p, g in zip(tparams.values(), grads):\n",
    "        m = theano.shared(p.get_value() * 0.)\n",
    "        v = theano.shared(p.get_value() * 0.)\n",
    "        m_t = (b1 * g) + ((1. - b1) * m)\n",
    "        v_t = (b2 * T.sqr(g)) + ((1. - b2) * v)\n",
    "        g_t = m_t / (T.sqrt(v_t) + e)\n",
    "        p_t = p - (lr_t * g_t)\n",
    "        updates.append((m, m_t))\n",
    "        updates.append((v, v_t))\n",
    "        updates.append((p, p_t))\n",
    "    updates.append((i, i_t))\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(tparams, options, W_emb=None):\n",
    "    keep_prob_emb = options['keepProbEmb']\n",
    "    keep_prob_context = options['keepProbContext']\n",
    "    alphaHiddenDimSize = options['alphaHiddenDimSize']\n",
    "    betaHiddenDimSize = options['betaHiddenDimSize']\n",
    "\n",
    "    trng = RandomStreams(1234)\n",
    "    use_noise = theano.shared(numpy_floatX(0.))\n",
    "    useTime = options['useTime']\n",
    "\n",
    "    x = T.tensor3('x', dtype=config.floatX)\n",
    "    t = T.matrix('t', dtype=config.floatX)\n",
    "    y = T.vector('y', dtype=config.floatX)\n",
    "    lengths = T.ivector('lengths')\n",
    "\n",
    "    n_timesteps = x.shape[0]\n",
    "    n_samples = x.shape[1]\n",
    "\n",
    "    if options['embFineTune']: emb = T.dot(x, tparams['W_emb'])\n",
    "    else: emb = T.dot(x, W_emb)\n",
    "\n",
    "    if keep_prob_emb < 1.0: emb = dropout_layer(emb, use_noise, trng, keep_prob_emb)\n",
    "\n",
    "    if useTime: temb = T.concatenate([emb, t.reshape([n_timesteps,n_samples,1])], axis=2) #Adding the time element to the embedding\n",
    "    else: temb = emb\n",
    "\n",
    "    def attentionStep(att_timesteps):\n",
    "        reverse_emb_t = temb[:att_timesteps][::-1]\n",
    "        reverse_h_a = gru_layer(tparams, reverse_emb_t, 'a', alphaHiddenDimSize)[::-1] * 0.5\n",
    "        reverse_h_b = gru_layer(tparams, reverse_emb_t, 'b', betaHiddenDimSize)[::-1] * 0.5\n",
    "\n",
    "        preAlpha = T.dot(reverse_h_a, tparams['w_alpha']) + tparams['b_alpha']\n",
    "        preAlpha = preAlpha.reshape((preAlpha.shape[0], preAlpha.shape[1]))\n",
    "        alpha = (T.nnet.softmax(preAlpha.T)).T\n",
    "\n",
    "        beta = T.tanh(T.dot(reverse_h_b, tparams['W_beta']) + tparams['b_beta'])\n",
    "        c_t = (alpha[:,:,None] * beta * emb[:att_timesteps]).sum(axis=0)\n",
    "        return c_t\n",
    "\n",
    "    counts = T.arange(n_timesteps)+ 1\n",
    "    c_t, updates = theano.scan(fn=attentionStep, sequences=[counts], outputs_info=None, name='attention_layer', n_steps=n_timesteps)\n",
    "    if keep_prob_context < 1.0: c_t = dropout_layer(c_t, use_noise, trng, keep_prob_context)\n",
    "\n",
    "    preY = T.nnet.sigmoid(T.dot(c_t, tparams['w_output']) + tparams['b_output'])\n",
    "    preY = preY.reshape((preY.shape[0], preY.shape[1]))\n",
    "    indexRow = T.arange(n_samples)\n",
    "    y_hat = preY.T[indexRow, lengths - 1]\n",
    "\n",
    "    logEps = options['logEps']\n",
    "    cross_entropy = -(y * T.log(y_hat + logEps) + (1. - y) * T.log(1. - y_hat + logEps))\n",
    "    cost_noreg = T.mean(cross_entropy)\n",
    "\n",
    "    cost = cost_noreg + options['L2_output'] * (tparams['w_output']**2).sum() + options['L2_alpha'] * (tparams['w_alpha']**2).sum() + options['L2_beta'] * (tparams['W_beta']**2).sum()\n",
    "\n",
    "    if options['embFineTune']: cost += options['L2_emb'] * (tparams['W_emb']**2).sum()\n",
    "\n",
    "    if useTime: return use_noise, x, y, t, lengths, cost_noreg, cost, y_hat\n",
    "    else: return use_noise, x, y, lengths, cost_noreg, cost, y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RETAIN(\n",
    "    seqFile='seqFile.txt',\n",
    "    inputDimSize=20000,\n",
    "    labelFile='labelFile.txt',\n",
    "    numClass=1,\n",
    "    outFile='outFile.txt',\n",
    "    timeFile='',\n",
    "    modelFile='model.npz',\n",
    "    useLogTime=True,\n",
    "    embFile='embFile.txt',\n",
    "    embDimSize=128,\n",
    "    embFineTune=True,\n",
    "    alphaHiddenDimSize=128,\n",
    "    betaHiddenDimSize=128,\n",
    "    batchSize=100,\n",
    "    max_epochs=10,\n",
    "    L2_output=0.001,\n",
    "    L2_emb=0.001,\n",
    "    L2_alpha=0.001,\n",
    "    L2_beta=0.001,\n",
    "    keepProbEmb=0.5,\n",
    "    keepProbContext=0.5,\n",
    "    logEps=1e-8,\n",
    "    solver='adadelta',\n",
    "    simpleLoad=False,\n",
    "    verbose=False\n",
    "):\n",
    "    options = locals().copy()\n",
    "\n",
    "    if len(timeFile) > 0: useTime = True\n",
    "    else: useTime = False\n",
    "    options['useTime'] = useTime\n",
    "\n",
    "    print 'Initializing the parameters ... ',\n",
    "    params = init_params(options)\n",
    "    if len(modelFile) > 0: params = load_params(options)\n",
    "    tparams = init_tparams(params, options)\n",
    "\n",
    "    print 'Building the model ... ',\n",
    "    if useTime and embFineTune:\n",
    "        print 'using time information, fine-tuning code representations'\n",
    "        use_noise, x, y, t, lengths, cost_noreg, cost, y_hat =  build_model(tparams, options)\n",
    "        if solver=='adadelta':\n",
    "            grads = T.grad(cost, wrt=tparams.values())\n",
    "            f_grad_shared, f_update = adadelta(tparams, grads, x, y, lengths, cost, options, t)\n",
    "        elif solver=='adam':\n",
    "            updates = adam(cost, tparams)\n",
    "            update_model = theano.function(inputs=[x, y, t, lengths], outputs=cost, updates=updates, name='update_model')\n",
    "        get_prediction = theano.function(inputs=[x, t, lengths], outputs=y_hat, name='get_prediction')\n",
    "        get_cost = theano.function(inputs=[x, y, t, lengths], outputs=cost_noreg, name='get_cost')\n",
    "    elif useTime and not embFineTune:\n",
    "        print 'using time information, not fine-tuning code representations'\n",
    "        W_emb = theano.shared(params['W_emb'], name='W_emb')\n",
    "        use_noise, x, y, t, lengths, cost_noreg, cost, y_hat =  build_model(tparams, options, W_emb)\n",
    "        if solver=='adadelta':\n",
    "            grads = T.grad(cost, wrt=tparams.values())\n",
    "            f_grad_shared, f_update = adadelta(tparams, grads, x, y, lengths, cost, options, t)\n",
    "        elif solver=='adam':\n",
    "            updates = adam(cost, tparams)\n",
    "            update_model = theano.function(inputs=[x, y, t, lengths], outputs=cost, updates=updates, name='update_model')\n",
    "        get_prediction = theano.function(inputs=[x, t, lengths], outputs=y_hat, name='get_prediction')\n",
    "        get_cost = theano.function(inputs=[x, y, t, lengths], outputs=cost_noreg, name='get_cost')\n",
    "    elif not useTime and embFineTune:\n",
    "        print 'not using time information, fine-tuning code representations'\n",
    "        use_noise, x, y, lengths, cost_noreg, cost, y_hat =  build_model(tparams, options)\n",
    "        if solver=='adadelta':\n",
    "            grads = T.grad(cost, wrt=tparams.values())\n",
    "            f_grad_shared, f_update = adadelta(tparams, grads, x, y, lengths, cost, options)\n",
    "        elif solver=='adam':\n",
    "            updates = adam(cost, tparams)\n",
    "            update_model = theano.function(inputs=[x, y, lengths], outputs=cost, updates=updates, name='update_model')\n",
    "        get_prediction = theano.function(inputs=[x, lengths], outputs=y_hat, name='get_prediction')\n",
    "        get_cost = theano.function(inputs=[x, y, lengths], outputs=cost_noreg, name='get_cost')\n",
    "    elif not useTime and not embFineTune:\n",
    "        print 'not using time information, not fine-tuning code representations'\n",
    "        W_emb = theano.shared(params['W_emb'], name='W_emb')\n",
    "        use_noise, x, y, lengths, cost_noreg, cost, y_hat =  build_model(tparams, options, W_emb)\n",
    "        if solver=='adadelta':\n",
    "            grads = T.grad(cost, wrt=tparams.values())\n",
    "            f_grad_shared, f_update = adadelta(tparams, grads, x, y, lengths, cost, options)\n",
    "        elif solver=='adam':\n",
    "            updates = adam(cost, tparams)\n",
    "            update_model = theano.function(inputs=[x, y, lengths], outputs=cost, updates=updates, name='update_model')\n",
    "        get_prediction = theano.function(inputs=[x, lengths], outputs=y_hat, name='get_prediction')\n",
    "        get_cost = theano.function(inputs=[x, y, lengths], outputs=cost_noreg, name='get_cost')\n",
    "\n",
    "    print 'Loading data ... ',\n",
    "    if simpleLoad:\n",
    "        trainSet, validSet, testSet = load_data_simple(seqFile, labelFile, timeFile)\n",
    "    else:\n",
    "        trainSet, validSet, testSet = load_data(seqFile, labelFile, timeFile)\n",
    "    n_batches = int(np.ceil(float(len(trainSet[0])) / float(batchSize)))\n",
    "    print 'done'\n",
    "\n",
    "    bestValidAuc = 0.0\n",
    "    bestTestAuc = 0.0\n",
    "    bestValidEpoch = 0\n",
    "    logFile = outFile + '.log'\n",
    "    print 'Optimization start !!'\n",
    "    for epoch in xrange(max_epochs):\n",
    "        iteration = 0\n",
    "        costVector = []\n",
    "        for index in random.sample(range(n_batches), n_batches):\n",
    "            use_noise.set_value(1.)\n",
    "            batchX = trainSet[0][index*batchSize:(index+1)*batchSize]\n",
    "            y = np.array(trainSet[1][index*batchSize:(index+1)*batchSize]).astype(config.floatX)\n",
    "\n",
    "            if useTime:\n",
    "                batchT = trainSet[2][index*batchSize:(index+1)*batchSize]\n",
    "                x, t, lengths = padMatrixWithTime(batchX, batchT, options)\n",
    "                if solver=='adadelta':\n",
    "                    costValue = f_grad_shared(x, y, t, lengths)\n",
    "                    f_update()\n",
    "                elif solver=='adam':\n",
    "                    costValue = update_model(x, y, t, lengths)\n",
    "            else:\n",
    "                x, lengths = padMatrixWithoutTime(batchX, options)\n",
    "                if solver=='adadelta':\n",
    "                    costValue = f_grad_shared(x, y, lengths)\n",
    "                    f_update()\n",
    "                elif solver=='adam':\n",
    "                    costValue = update_model(x, y, lengths)\n",
    "            costVector.append(costValue)\n",
    "            if (iteration % 10 == 0) and verbose: \n",
    "                print 'Epoch:%d, Iteration:%d/%d, Train_Cost:%f' % (epoch, iteration, n_batches, costValue)\n",
    "            iteration += 1\n",
    "\n",
    "        use_noise.set_value(0.)\n",
    "        trainCost = np.mean(costVector)\n",
    "        validAuc = calculate_auc(get_prediction, validSet, options)\n",
    "        buf = 'Epoch:%d, Train_cost:%f, Validation_AUC:%f' % (epoch, trainCost, validAuc)\n",
    "        print buf\n",
    "        print2file(buf, logFile)\n",
    "        if validAuc > bestValidAuc: \n",
    "            bestValidAuc = validAuc\n",
    "            bestValidEpoch = epoch\n",
    "            bestTestAuc = calculate_auc(get_prediction, testSet, options)\n",
    "            buf = 'Currently the best validation AUC found. Test AUC:%f at epoch:%d' % (bestTestAuc, epoch)\n",
    "            print buf\n",
    "            print2file(buf, logFile)\n",
    "            tempParams = unzip(tparams)\n",
    "            np.savez_compressed(outFile + '.' + str(epoch), **tempParams)\n",
    "    buf = 'The best validation & test AUC:%f, %f at epoch:%d' % (bestValidAuc, bestTestAuc, bestValidEpoch)\n",
    "    print buf\n",
    "    print2file(buf, logFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the parameters ...  using randomly initialized code embedding\n",
      "Building the model ...  not using time information, fine-tuning code representations\n",
      "Loading data ...  done\n",
      "Optimization start !!\n",
      "Epoch:0, Iteration:0/53, Train_Cost:9.282189\n",
      "Epoch:0, Iteration:10/53, Train_Cost:8.944542\n",
      "Epoch:0, Iteration:20/53, Train_Cost:8.610385\n",
      "Epoch:0, Iteration:30/53, Train_Cost:8.300296\n",
      "Epoch:0, Iteration:40/53, Train_Cost:7.979092\n",
      "Epoch:0, Iteration:50/53, Train_Cost:7.712511\n",
      "Epoch:0, Train_cost:8.435225, Validation_AUC:0.538697\n",
      "Currently the best validation AUC found. Test AUC:0.575994 at epoch:0\n",
      "Epoch:1, Iteration:0/53, Train_Cost:7.619331\n",
      "Epoch:1, Iteration:10/53, Train_Cost:7.362564\n",
      "Epoch:1, Iteration:20/53, Train_Cost:7.116832\n",
      "Epoch:1, Iteration:30/53, Train_Cost:6.813944\n",
      "Epoch:1, Iteration:40/53, Train_Cost:6.591929\n",
      "Epoch:1, Iteration:50/53, Train_Cost:6.368921\n",
      "Epoch:1, Train_cost:6.939371, Validation_AUC:0.559460\n",
      "Currently the best validation AUC found. Test AUC:0.598259 at epoch:1\n",
      "Epoch:2, Iteration:0/53, Train_Cost:6.249791\n",
      "Epoch:2, Iteration:10/53, Train_Cost:6.034783\n",
      "Epoch:2, Iteration:20/53, Train_Cost:5.837225\n",
      "Epoch:2, Iteration:30/53, Train_Cost:5.612291\n",
      "Epoch:2, Iteration:40/53, Train_Cost:5.401036\n",
      "Epoch:2, Iteration:50/53, Train_Cost:5.259605\n",
      "Epoch:2, Train_cost:5.735474, Validation_AUC:0.597402\n",
      "Currently the best validation AUC found. Test AUC:0.641201 at epoch:2\n",
      "Epoch:3, Iteration:0/53, Train_Cost:5.191360\n",
      "Epoch:3, Iteration:10/53, Train_Cost:4.990907\n",
      "Epoch:3, Iteration:20/53, Train_Cost:4.886774\n",
      "Epoch:3, Iteration:30/53, Train_Cost:4.652915\n",
      "Epoch:3, Iteration:40/53, Train_Cost:4.560853\n",
      "Epoch:3, Iteration:50/53, Train_Cost:4.383774\n",
      "Epoch:3, Train_cost:4.761835, Validation_AUC:0.646880\n",
      "Currently the best validation AUC found. Test AUC:0.694729 at epoch:3\n",
      "Epoch:4, Iteration:0/53, Train_Cost:4.323982\n",
      "Epoch:4, Iteration:10/53, Train_Cost:4.174005\n",
      "Epoch:4, Iteration:20/53, Train_Cost:3.982266\n",
      "Epoch:4, Iteration:30/53, Train_Cost:3.866151\n",
      "Epoch:4, Iteration:40/53, Train_Cost:3.790223\n",
      "Epoch:4, Iteration:50/53, Train_Cost:3.643109\n",
      "Epoch:4, Train_cost:3.963128, Validation_AUC:0.686805\n",
      "Currently the best validation AUC found. Test AUC:0.728686 at epoch:4\n",
      "Epoch:5, Iteration:0/53, Train_Cost:3.600524\n",
      "Epoch:5, Iteration:10/53, Train_Cost:3.467901\n",
      "Epoch:5, Iteration:20/53, Train_Cost:3.424363\n",
      "Epoch:5, Iteration:30/53, Train_Cost:3.271043\n",
      "Epoch:5, Iteration:40/53, Train_Cost:3.033257\n",
      "Epoch:5, Iteration:50/53, Train_Cost:3.056312\n",
      "Epoch:5, Train_cost:3.300060, Validation_AUC:0.713453\n",
      "Currently the best validation AUC found. Test AUC:0.753629 at epoch:5\n",
      "Epoch:6, Iteration:0/53, Train_Cost:3.062808\n",
      "Epoch:6, Iteration:10/53, Train_Cost:2.943793\n",
      "Epoch:6, Iteration:20/53, Train_Cost:2.806942\n",
      "Epoch:6, Iteration:30/53, Train_Cost:2.694008\n",
      "Epoch:6, Iteration:40/53, Train_Cost:2.668524\n",
      "Epoch:6, Iteration:50/53, Train_Cost:2.458050\n",
      "Epoch:6, Train_cost:2.765350, Validation_AUC:0.735879\n",
      "Currently the best validation AUC found. Test AUC:0.775692 at epoch:6\n",
      "Epoch:7, Iteration:0/53, Train_Cost:2.543513\n",
      "Epoch:7, Iteration:10/53, Train_Cost:2.466351\n",
      "Epoch:7, Iteration:20/53, Train_Cost:2.399794\n",
      "Epoch:7, Iteration:30/53, Train_Cost:2.348476\n",
      "Epoch:7, Iteration:40/53, Train_Cost:2.276413\n",
      "Epoch:7, Iteration:50/53, Train_Cost:2.232352\n",
      "Epoch:7, Train_cost:2.339284, Validation_AUC:0.754855\n",
      "Currently the best validation AUC found. Test AUC:0.793005 at epoch:7\n",
      "Epoch:8, Iteration:0/53, Train_Cost:2.037253\n",
      "Epoch:8, Iteration:10/53, Train_Cost:2.105666\n",
      "Epoch:8, Iteration:20/53, Train_Cost:2.012584\n",
      "Epoch:8, Iteration:30/53, Train_Cost:1.937858\n",
      "Epoch:8, Iteration:40/53, Train_Cost:1.961287\n",
      "Epoch:8, Iteration:50/53, Train_Cost:1.820844\n",
      "Epoch:8, Train_cost:1.990281, Validation_AUC:0.768178\n",
      "Currently the best validation AUC found. Test AUC:0.802045 at epoch:8\n",
      "Epoch:9, Iteration:0/53, Train_Cost:1.837152\n",
      "Epoch:9, Iteration:10/53, Train_Cost:1.808715\n",
      "Epoch:9, Iteration:20/53, Train_Cost:1.685372\n",
      "Epoch:9, Iteration:30/53, Train_Cost:1.679390\n",
      "Epoch:9, Iteration:40/53, Train_Cost:1.693586\n",
      "Epoch:9, Iteration:50/53, Train_Cost:1.595401\n",
      "Epoch:9, Train_cost:1.711632, Validation_AUC:0.763211\n",
      "Epoch:10, Iteration:0/53, Train_Cost:1.539168\n",
      "Epoch:10, Iteration:10/53, Train_Cost:1.474186\n",
      "Epoch:10, Iteration:20/53, Train_Cost:1.461063\n",
      "Epoch:10, Iteration:30/53, Train_Cost:1.550935\n",
      "Epoch:10, Iteration:40/53, Train_Cost:1.297986\n",
      "Epoch:10, Iteration:50/53, Train_Cost:1.424751\n",
      "Epoch:10, Train_cost:1.483265, Validation_AUC:0.772415\n",
      "Currently the best validation AUC found. Test AUC:0.804585 at epoch:10\n",
      "Epoch:11, Iteration:0/53, Train_Cost:1.335967\n",
      "Epoch:11, Iteration:10/53, Train_Cost:1.313874\n",
      "Epoch:11, Iteration:20/53, Train_Cost:1.329942\n",
      "Epoch:11, Iteration:30/53, Train_Cost:1.242953\n",
      "Epoch:11, Iteration:40/53, Train_Cost:1.319152\n",
      "Epoch:11, Iteration:50/53, Train_Cost:1.205180\n",
      "Epoch:11, Train_cost:1.297487, Validation_AUC:0.777281\n",
      "Currently the best validation AUC found. Test AUC:0.809343 at epoch:11\n",
      "Epoch:12, Iteration:0/53, Train_Cost:1.165482\n",
      "Epoch:12, Iteration:10/53, Train_Cost:1.191936\n",
      "Epoch:12, Iteration:20/53, Train_Cost:1.110648\n",
      "Epoch:12, Iteration:30/53, Train_Cost:1.204595\n",
      "Epoch:12, Iteration:40/53, Train_Cost:1.068268\n",
      "Epoch:12, Iteration:50/53, Train_Cost:1.075078\n",
      "Epoch:12, Train_cost:1.145853, Validation_AUC:0.777483\n",
      "Currently the best validation AUC found. Test AUC:0.807080 at epoch:12\n",
      "Epoch:13, Iteration:0/53, Train_Cost:0.997962\n",
      "Epoch:13, Iteration:10/53, Train_Cost:1.119937\n",
      "Epoch:13, Iteration:20/53, Train_Cost:1.050372\n",
      "Epoch:13, Iteration:30/53, Train_Cost:0.929903\n",
      "Epoch:13, Iteration:40/53, Train_Cost:0.995023\n",
      "Epoch:13, Iteration:50/53, Train_Cost:0.965306\n",
      "Epoch:13, Train_cost:1.027841, Validation_AUC:0.782761\n",
      "Currently the best validation AUC found. Test AUC:0.808351 at epoch:13\n",
      "Epoch:14, Iteration:0/53, Train_Cost:0.904328\n",
      "Epoch:14, Iteration:10/53, Train_Cost:0.896144\n",
      "Epoch:14, Iteration:20/53, Train_Cost:0.962916\n",
      "Epoch:14, Iteration:30/53, Train_Cost:0.894948\n",
      "Epoch:14, Iteration:40/53, Train_Cost:0.978924\n",
      "Epoch:14, Iteration:50/53, Train_Cost:0.880112\n",
      "Epoch:14, Train_cost:0.924834, Validation_AUC:0.776644\n",
      "Epoch:15, Iteration:0/53, Train_Cost:0.830733\n",
      "Epoch:15, Iteration:10/53, Train_Cost:0.979718\n",
      "Epoch:15, Iteration:20/53, Train_Cost:0.842558\n",
      "Epoch:15, Iteration:30/53, Train_Cost:0.939939\n",
      "Epoch:15, Iteration:40/53, Train_Cost:0.821963\n",
      "Epoch:15, Iteration:50/53, Train_Cost:0.696197\n",
      "Epoch:15, Train_cost:0.850833, Validation_AUC:0.779559\n",
      "Epoch:16, Iteration:0/53, Train_Cost:0.834256\n",
      "Epoch:16, Iteration:10/53, Train_Cost:0.852982\n",
      "Epoch:16, Iteration:20/53, Train_Cost:0.728613\n",
      "Epoch:16, Iteration:30/53, Train_Cost:0.822691\n",
      "Epoch:16, Iteration:40/53, Train_Cost:0.847669\n",
      "Epoch:16, Iteration:50/53, Train_Cost:0.873806\n",
      "Epoch:16, Train_cost:0.782334, Validation_AUC:0.779753\n",
      "Epoch:17, Iteration:0/53, Train_Cost:0.678226\n",
      "Epoch:17, Iteration:10/53, Train_Cost:0.726810\n",
      "Epoch:17, Iteration:20/53, Train_Cost:0.744995\n",
      "Epoch:17, Iteration:30/53, Train_Cost:0.706794\n",
      "Epoch:17, Iteration:40/53, Train_Cost:0.792233\n",
      "Epoch:17, Iteration:50/53, Train_Cost:0.788152\n",
      "Epoch:17, Train_cost:0.734148, Validation_AUC:0.785746\n",
      "Currently the best validation AUC found. Test AUC:0.813345 at epoch:17\n",
      "Epoch:18, Iteration:0/53, Train_Cost:0.712701\n",
      "Epoch:18, Iteration:10/53, Train_Cost:0.665744\n",
      "Epoch:18, Iteration:20/53, Train_Cost:0.704500\n",
      "Epoch:18, Iteration:30/53, Train_Cost:0.641083\n",
      "Epoch:18, Iteration:40/53, Train_Cost:0.708816\n",
      "Epoch:18, Iteration:50/53, Train_Cost:0.652537\n",
      "Epoch:18, Train_cost:0.689128, Validation_AUC:0.783150\n",
      "Epoch:19, Iteration:0/53, Train_Cost:0.658634\n",
      "Epoch:19, Iteration:10/53, Train_Cost:0.628945\n",
      "Epoch:19, Iteration:20/53, Train_Cost:0.706884\n",
      "Epoch:19, Iteration:30/53, Train_Cost:0.717512\n",
      "Epoch:19, Iteration:40/53, Train_Cost:0.666038\n",
      "Epoch:19, Iteration:50/53, Train_Cost:0.770926\n",
      "Epoch:19, Train_cost:0.654984, Validation_AUC:0.789796\n",
      "Currently the best validation AUC found. Test AUC:0.817760 at epoch:19\n",
      "Epoch:20, Iteration:0/53, Train_Cost:0.597044\n",
      "Epoch:20, Iteration:10/53, Train_Cost:0.655533\n",
      "Epoch:20, Iteration:20/53, Train_Cost:0.644315\n",
      "Epoch:20, Iteration:30/53, Train_Cost:0.542675\n",
      "Epoch:20, Iteration:40/53, Train_Cost:0.802568\n",
      "Epoch:20, Iteration:50/53, Train_Cost:0.588428\n",
      "Epoch:20, Train_cost:0.626759, Validation_AUC:0.788848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:21, Iteration:0/53, Train_Cost:0.585728\n",
      "Epoch:21, Iteration:10/53, Train_Cost:0.555573\n",
      "Epoch:21, Iteration:20/53, Train_Cost:0.598554\n",
      "Epoch:21, Iteration:30/53, Train_Cost:0.615772\n",
      "Epoch:21, Iteration:40/53, Train_Cost:0.625227\n",
      "Epoch:21, Iteration:50/53, Train_Cost:0.627084\n",
      "Epoch:21, Train_cost:0.597903, Validation_AUC:0.790380\n",
      "Currently the best validation AUC found. Test AUC:0.821033 at epoch:21\n",
      "Epoch:22, Iteration:0/53, Train_Cost:0.561705\n",
      "Epoch:22, Iteration:10/53, Train_Cost:0.639494\n",
      "Epoch:22, Iteration:20/53, Train_Cost:0.689874\n",
      "Epoch:22, Iteration:30/53, Train_Cost:0.636164\n",
      "Epoch:22, Iteration:40/53, Train_Cost:0.564379\n",
      "Epoch:22, Iteration:50/53, Train_Cost:0.520079\n",
      "Epoch:22, Train_cost:0.579928, Validation_AUC:0.789198\n",
      "Epoch:23, Iteration:0/53, Train_Cost:0.584251\n",
      "Epoch:23, Iteration:10/53, Train_Cost:0.546450\n",
      "Epoch:23, Iteration:20/53, Train_Cost:0.518027\n",
      "Epoch:23, Iteration:30/53, Train_Cost:0.570580\n",
      "Epoch:23, Iteration:40/53, Train_Cost:0.646794\n",
      "Epoch:23, Iteration:50/53, Train_Cost:0.547298\n",
      "Epoch:23, Train_cost:0.569571, Validation_AUC:0.788949\n",
      "Epoch:24, Iteration:0/53, Train_Cost:0.601598\n",
      "Epoch:24, Iteration:10/53, Train_Cost:0.459221\n",
      "Epoch:24, Iteration:20/53, Train_Cost:0.525437\n",
      "Epoch:24, Iteration:30/53, Train_Cost:0.589673\n",
      "Epoch:24, Iteration:40/53, Train_Cost:0.502743\n",
      "Epoch:24, Iteration:50/53, Train_Cost:0.439103\n",
      "Epoch:24, Train_cost:0.553606, Validation_AUC:0.787503\n",
      "Epoch:25, Iteration:0/53, Train_Cost:0.577745\n",
      "Epoch:25, Iteration:10/53, Train_Cost:0.500640\n",
      "Epoch:25, Iteration:20/53, Train_Cost:0.528285\n",
      "Epoch:25, Iteration:30/53, Train_Cost:0.514943\n",
      "Epoch:25, Iteration:40/53, Train_Cost:0.525126\n",
      "Epoch:25, Iteration:50/53, Train_Cost:0.683495\n",
      "Epoch:25, Train_cost:0.542791, Validation_AUC:0.793015\n",
      "Currently the best validation AUC found. Test AUC:0.819354 at epoch:25\n",
      "Epoch:26, Iteration:0/53, Train_Cost:0.580520\n",
      "Epoch:26, Iteration:10/53, Train_Cost:0.587000\n",
      "Epoch:26, Iteration:20/53, Train_Cost:0.449443\n",
      "Epoch:26, Iteration:30/53, Train_Cost:0.614304\n",
      "Epoch:26, Iteration:40/53, Train_Cost:0.496263\n",
      "Epoch:26, Iteration:50/53, Train_Cost:0.606702\n",
      "Epoch:26, Train_cost:0.532912, Validation_AUC:0.790177\n",
      "Epoch:27, Iteration:0/53, Train_Cost:0.563738\n",
      "Epoch:27, Iteration:10/53, Train_Cost:0.396857\n",
      "Epoch:27, Iteration:20/53, Train_Cost:0.404801\n",
      "Epoch:27, Iteration:30/53, Train_Cost:0.483833\n",
      "Epoch:27, Iteration:40/53, Train_Cost:0.534622\n",
      "Epoch:27, Iteration:50/53, Train_Cost:0.458596\n",
      "Epoch:27, Train_cost:0.519674, Validation_AUC:0.784153\n",
      "Epoch:28, Iteration:0/53, Train_Cost:0.550442\n",
      "Epoch:28, Iteration:10/53, Train_Cost:0.541486\n",
      "Epoch:28, Iteration:20/53, Train_Cost:0.565683\n",
      "Epoch:28, Iteration:30/53, Train_Cost:0.424023\n",
      "Epoch:28, Iteration:40/53, Train_Cost:0.400419\n",
      "Epoch:28, Iteration:50/53, Train_Cost:0.518295\n",
      "Epoch:28, Train_cost:0.510323, Validation_AUC:0.794212\n",
      "Currently the best validation AUC found. Test AUC:0.818321 at epoch:28\n",
      "Epoch:29, Iteration:0/53, Train_Cost:0.514076\n",
      "Epoch:29, Iteration:10/53, Train_Cost:0.489724\n",
      "Epoch:29, Iteration:20/53, Train_Cost:0.439243\n",
      "Epoch:29, Iteration:30/53, Train_Cost:0.615233\n",
      "Epoch:29, Iteration:40/53, Train_Cost:0.468610\n",
      "Epoch:29, Iteration:50/53, Train_Cost:0.395526\n",
      "Epoch:29, Train_cost:0.513500, Validation_AUC:0.789159\n",
      "The best validation & test AUC:0.794212, 0.818321 at epoch:28\n"
     ]
    }
   ],
   "source": [
    "train_RETAIN(\n",
    "    seqFile=seqFile, \n",
    "    inputDimSize=inputDimSize, \n",
    "    labelFile=labelFile, \n",
    "    numClass=numClass, \n",
    "    outFile=outFile, \n",
    "    timeFile=timeFile, \n",
    "    modelFile=modelFile,\n",
    "    useLogTime=useLogTime,\n",
    "    embFile=embFile, \n",
    "    embDimSize=embDimSize, \n",
    "    embFineTune=embFineTune, \n",
    "    alphaHiddenDimSize=alphaHiddenDimSize,\n",
    "    betaHiddenDimSize=betaHiddenDimSize,\n",
    "    batchSize=batchSize, \n",
    "    max_epochs=max_epochs, \n",
    "    L2_output=L2_output, \n",
    "    L2_emb=L2_emb, \n",
    "    L2_alpha=L2_alpha, \n",
    "    L2_beta=L2_beta, \n",
    "    keepProbEmb=keepProbEmb, \n",
    "    keepProbContext=keepProbContext, \n",
    "    logEps=logEps, \n",
    "    solver=solver,\n",
    "    simpleLoad=simpleLoad,\n",
    "    verbose=verbose\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
