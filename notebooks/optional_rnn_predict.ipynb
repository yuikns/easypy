{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOESN'T WORK NOW\n",
    "\n",
    "\n",
    "# RNN Predict\n",
    "\n",
    "\n",
    "\n",
    "From <https://github.com/mp2893/rnn_predict>\n",
    "\n",
    "This is a simple RNN (implemented with Gated Recurrent Units) for predicting a HF diagnosis given patient records. There are four different versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseDir = \"/work/data/\"\n",
    "generatedDir = baseDir + \"generated/\"\n",
    "fileBase = generatedDir + \"mimic3\"\n",
    "\n",
    "# sequences.pkl: This is a pickled list of list of integers. Each integer is assumed to be some medical code.\n",
    "# times.pkl: This is a pickled list of list of integers. Each integer is assumed to the time at which the medical code occurred.\n",
    "# labels.pkl: This is a pickled list of 0 and 1s.\n",
    "# emb.pkl: This is a randomly generated code embedding of size 100 X 100\n",
    "\n",
    "# type=str\n",
    "# The path to the Pickled file containing visit information of patients\n",
    "seqFile = fileBase + '.seqs'\n",
    "\n",
    "# type=str\n",
    "# The path to the Pickled file containing label information of patients\n",
    "labelFile = fileBase +'.morts'\n",
    "\n",
    "# The path to the Pickled file containing durations between visits of patients. \n",
    "# If you are not using duration information, do not use this option\n",
    "timeFile = fileBase + '.dates'\n",
    "\n",
    "\n",
    "# The path to the output models. The models will be saved after every epoch\n",
    "# outFile= fileBase + '.model'\n",
    "\n",
    "# type=str\n",
    "# The path to the Numpy-compressed file containing the model parameters. \n",
    "# Use this option if you want to re-train an existing model\n",
    "# bestEpoch = 22\n",
    "# modelFile= fileBase + '.model.' + str(bestEpoch) + '.npz'\n",
    "modelFile= ''\n",
    "\n",
    "\n",
    "simpleLoad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_TEST_RATIO = 0.2\n",
    "_VALIDATION_RATIO = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, random\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano import config\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "import cPickle as pickle\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip(zipped):\n",
    "    new_params = OrderedDict()\n",
    "    for key, value in zipped.iteritems():\n",
    "        new_params[key] = value.get_value()\n",
    "    return new_params\n",
    "\n",
    "def numpy_floatX(data):\n",
    "    return np.asarray(data, dtype=config.floatX)\n",
    "\n",
    "def get_random_weight(dim1, dim2, left=-0.1, right=0.1):\n",
    "    return np.random.uniform(left, right, (dim1, dim2)).astype(config.floatX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(infile):\n",
    "    Wemb = np.array(pickle.load(open(infile, 'rb'))).astype(config.floatX)\n",
    "    return Wemb\n",
    "\n",
    "def padMatrixWithTime(seqs, times, options):\n",
    "    lengths = np.array([len(seq) for seq in seqs]).astype('int32')\n",
    "    n_samples = len(seqs)\n",
    "    maxlen = np.max(lengths)\n",
    "\n",
    "    x = np.zeros((maxlen, n_samples, options['inputDimSize'])).astype(config.floatX)\n",
    "    t = np.zeros((maxlen, n_samples)).astype(config.floatX)\n",
    "    for idx, (seq,time) in enumerate(zip(seqs,times)):\n",
    "        for xvec, subseq in zip(x[:,idx,:], seq):\n",
    "            xvec[subseq] = 1.\n",
    "        t[:lengths[idx], idx] = time\n",
    "\n",
    "    if options['useLogTime']: t = np.log(t + 1.)\n",
    "\n",
    "    return x, t, lengths\n",
    "\n",
    "def padMatrixWithoutTime(seqs, options):\n",
    "    lengths = np.array([len(seq) for seq in seqs]).astype('int32')\n",
    "    n_samples = len(seqs)\n",
    "    maxlen = np.max(lengths)\n",
    "\n",
    "    x = np.zeros((maxlen, n_samples, options['inputDimSize'])).astype(config.floatX)\n",
    "    for idx, seq in enumerate(seqs):\n",
    "        for xvec, subseq in zip(x[:,idx,:], seq):\n",
    "            xvec[subseq] = 1.\n",
    "\n",
    "    return x, lengths\n",
    "\n",
    "def load_data_simple(seqFile, labelFile, timeFile=''):\n",
    "    sequences = np.array(pickle.load(open(seqFile, 'rb')))\n",
    "    labels = np.array(pickle.load(open(labelFile, 'rb')))\n",
    "    if len(timeFile) > 0:\n",
    "        times = np.array(pickle.load(open(timeFile, 'rb')))\n",
    "\n",
    "    dataSize = len(labels)\n",
    "    np.random.seed(0)\n",
    "    ind = np.random.permutation(dataSize)\n",
    "    nTest = int(_TEST_RATIO * dataSize)\n",
    "    nValid = int(_VALIDATION_RATIO * dataSize)\n",
    "\n",
    "    test_indices = ind[:nTest]\n",
    "    valid_indices = ind[nTest:nTest+nValid]\n",
    "    train_indices = ind[nTest+nValid:]\n",
    "\n",
    "    train_set_x = sequences[train_indices]\n",
    "    train_set_y = labels[train_indices]\n",
    "    test_set_x = sequences[test_indices]\n",
    "    test_set_y = labels[test_indices]\n",
    "    valid_set_x = sequences[valid_indices]\n",
    "    valid_set_y = labels[valid_indices]\n",
    "    train_set_t = None\n",
    "    test_set_t = None\n",
    "    valid_set_t = None\n",
    "\n",
    "    if len(timeFile) > 0:\n",
    "        train_set_t = times[train_indices]\n",
    "        test_set_t = times[test_indices]\n",
    "        valid_set_t = times[valid_indices]\n",
    "\n",
    "    def len_argsort(seq):\n",
    "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "\n",
    "    train_sorted_index = len_argsort(train_set_x)\n",
    "    train_set_x = [train_set_x[i] for i in train_sorted_index]\n",
    "    train_set_y = [train_set_y[i] for i in train_sorted_index]\n",
    "\n",
    "    valid_sorted_index = len_argsort(valid_set_x)\n",
    "    valid_set_x = [valid_set_x[i] for i in valid_sorted_index]\n",
    "    valid_set_y = [valid_set_y[i] for i in valid_sorted_index]\n",
    "\n",
    "    test_sorted_index = len_argsort(test_set_x)\n",
    "    test_set_x = [test_set_x[i] for i in test_sorted_index]\n",
    "    test_set_y = [test_set_y[i] for i in test_sorted_index]\n",
    "\n",
    "    if len(timeFile) > 0:\n",
    "        train_set_t = [train_set_t[i] for i in train_sorted_index]\n",
    "        valid_set_t = [valid_set_t[i] for i in valid_sorted_index]\n",
    "        test_set_t = [test_set_t[i] for i in test_sorted_index]\n",
    "\n",
    "    train_set = (train_set_x, train_set_y, train_set_t)\n",
    "    valid_set = (valid_set_x, valid_set_y, valid_set_t)\n",
    "    test_set = (test_set_x, test_set_y, test_set_t)\n",
    "\n",
    "    return train_set, valid_set, test_set\n",
    "\n",
    "\n",
    "def load_data(seqFile, labelFile, timeFile):\n",
    "    train_set_x = pickle.load(open(seqFile+'.train', 'rb'))\n",
    "    valid_set_x = pickle.load(open(seqFile+'.valid', 'rb'))\n",
    "    test_set_x = pickle.load(open(seqFile+'.test', 'rb'))\n",
    "    train_set_y = pickle.load(open(labelFile+'.train', 'rb'))\n",
    "    valid_set_y = pickle.load(open(labelFile+'.valid', 'rb'))\n",
    "    test_set_y = pickle.load(open(labelFile+'.test', 'rb'))\n",
    "    train_set_t = None\n",
    "    valid_set_t = None\n",
    "    test_set_t = None\n",
    "\n",
    "    if len(timeFile) > 0:\n",
    "        train_set_t = pickle.load(open(timeFile+'.train', 'rb'))\n",
    "        valid_set_t = pickle.load(open(timeFile+'.valid', 'rb'))\n",
    "        test_set_t = pickle.load(open(timeFile+'.test', 'rb'))\n",
    "\n",
    "    def len_argsort(seq):\n",
    "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "\n",
    "    train_sorted_index = len_argsort(train_set_x)\n",
    "    train_set_x = [train_set_x[i] for i in train_sorted_index]\n",
    "    train_set_y = [train_set_y[i] for i in train_sorted_index]\n",
    "\n",
    "    valid_sorted_index = len_argsort(valid_set_x)\n",
    "    valid_set_x = [valid_set_x[i] for i in valid_sorted_index]\n",
    "    valid_set_y = [valid_set_y[i] for i in valid_sorted_index]\n",
    "\n",
    "    test_sorted_index = len_argsort(test_set_x)\n",
    "    test_set_x = [test_set_x[i] for i in test_sorted_index]\n",
    "    test_set_y = [test_set_y[i] for i in test_sorted_index]\n",
    "\n",
    "    if len(timeFile) > 0:\n",
    "        train_set_t = [train_set_t[i] for i in train_sorted_index]\n",
    "        valid_set_t = [valid_set_t[i] for i in valid_sorted_index]\n",
    "        test_set_t = [test_set_t[i] for i in test_sorted_index]\n",
    "\n",
    "    train_set = (train_set_x, train_set_y, train_set_t)\n",
    "    valid_set = (valid_set_x, valid_set_y, valid_set_t)\n",
    "    test_set = (test_set_x, test_set_y, test_set_t)\n",
    "\n",
    "    return train_set, valid_set, test_set\n",
    "\n",
    "def calculate_auc(test_model, dataset, options):\n",
    "    batchSize = options['batchSize']\n",
    "    useTime = options['useTime']\n",
    "\n",
    "    n_batches = int(np.ceil(float(len(dataset[0])) / float(batchSize)))\n",
    "    scoreVec = []\n",
    "    for index in xrange(n_batches):\n",
    "        batchX = dataset[0][index*batchSize:(index+1)*batchSize]\n",
    "        if useTime:\n",
    "            batchT = dataset[2][index*batchSize:(index+1)*batchSize]\n",
    "            x, t, lengths = padMatrixWithTime(batchX, batchT, options)\n",
    "            scores = test_model(x, t, lengths)\n",
    "        else:\n",
    "            x, lengths = padMatrixWithoutTime(batchX, options)\n",
    "            scores = test_model(x, lengths)\n",
    "        scoreVec.extend(list(scores))\n",
    "    labels = dataset[1]\n",
    "    auc = roc_auc_score(list(labels), list(scoreVec))\n",
    "    return auc\n",
    "\n",
    "def calculate_cost(test_model, dataset, options):\n",
    "    batchSize = options['batchSize']\n",
    "    useTime = options['useTime']\n",
    "\n",
    "    costSum = 0.0\n",
    "    dataCount = 0\n",
    "\n",
    "    n_batches = int(np.ceil(float(len(dataset[0])) / float(batchSize)))\n",
    "    for index in xrange(n_batches):\n",
    "        batchX = dataset[0][index*batchSize:(index+1)*batchSize]\n",
    "        if useTime:\n",
    "            batchT = dataset[2][index*batchSize:(index+1)*batchSize]\n",
    "            x, t, lengths = padMatrixWithTime(batchX, batchT, options)\n",
    "            y = np.array(dataset[1][index*batchSize:(index+1)*batchSize]).astype(config.floatX)\n",
    "            scores = test_model(x, y, t, lengths)\n",
    "        else:\n",
    "            x, lengths = padMatrixWithoutTime(batchX, options)\n",
    "            y = np.array(dataset[1][index*batchSize:(index+1)*batchSize]).astype(config.floatX)\n",
    "            scores = test_model(x, y, lengths)\n",
    "        costSum += scores * len(batchX)\n",
    "        dataCount += len(batchX)\n",
    "    return costSum / dataCount\n",
    "\n",
    "def print2file(buf, outFile):\n",
    "    outfd = open(outFile, 'a')\n",
    "    outfd.write(buf + '\\n')\n",
    "    outfd.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU OneHot\n",
    "\n",
    "```\n",
    "python gru_onehot.py sequences.pkl labels.pkl\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(options):\n",
    "    params = OrderedDict()\n",
    "\n",
    "    inputDimSize = options['inputDimSize'] \n",
    "    hiddenDimSize = options['hiddenDimSize'] #hidden layer does not need an extra space\n",
    "\n",
    "    params['W_emb'] = np.eye(inputDimSize, dtype=config.floatX)\n",
    "\n",
    "    params['W_gru'] = get_random_weight(inputDimSize, 3*hiddenDimSize)\n",
    "    params['U_gru'] = get_random_weight(hiddenDimSize, 3*hiddenDimSize)\n",
    "    params['b_gru'] = np.zeros(3*hiddenDimSize).astype(config.floatX)\n",
    "\n",
    "    params['W_logistic'] = get_random_weight(hiddenDimSize,1)\n",
    "    params['b_logistic'] = np.zeros((1,), dtype=config.floatX)\n",
    "\n",
    "    return params\n",
    "\n",
    "def init_tparams(params):\n",
    "    tparams = OrderedDict()\n",
    "    for key, value in params.iteritems():\n",
    "        if key == 'W_emb': continue#####################\n",
    "        tparams[key] = theano.shared(value, name=key)\n",
    "    return tparams\n",
    "\n",
    "def dropout_layer(state_before, use_noise, trng):\n",
    "    proj = T.switch(use_noise, (state_before * trng.binomial(state_before.shape, p=0.5, n=1, dtype=state_before.dtype)), state_before * 0.5)\n",
    "    return proj\n",
    "\n",
    "def _slice(_x, n, dim):\n",
    "    if _x.ndim == 3:\n",
    "        return _x[:, :, n*dim:(n+1)*dim]\n",
    "    return _x[:, n*dim:(n+1)*dim]\n",
    "\n",
    "def gru_layer(tparams, emb, options, mask=None):\n",
    "    hiddenDimSize = options['hiddenDimSize']\n",
    "    timesteps = emb.shape[0]\n",
    "    if emb.ndim == 3: n_samples = emb.shape[1]\n",
    "    else: n_samples = 1\n",
    "\n",
    "    def stepFn(stepMask, wx, h, U_gru):\n",
    "        uh = T.dot(h, U_gru)\n",
    "        r = T.nnet.sigmoid(_slice(wx, 0, hiddenDimSize) + _slice(uh, 0, hiddenDimSize))\n",
    "        z = T.nnet.sigmoid(_slice(wx, 1, hiddenDimSize) + _slice(uh, 1, hiddenDimSize))\n",
    "        h_tilde = T.tanh(_slice(wx, 2, hiddenDimSize) + r * _slice(uh, 2, hiddenDimSize))\n",
    "        h_new = z * h + ((1. - z) * h_tilde)\n",
    "        h_new = stepMask[:, None] * h_new + (1. - stepMask)[:, None] * h\n",
    "        return h_new\n",
    "\n",
    "    Wx = T.dot(emb, tparams['W_gru']) + tparams['b_gru']\n",
    "    results, updates = theano.scan(fn=stepFn, sequences=[mask,Wx], outputs_info=T.alloc(numpy_floatX(0.0), n_samples, hiddenDimSize), non_sequences=[tparams['U_gru']], name='gru_layer', n_steps=timesteps)\n",
    "\n",
    "    return results[-1] #We only care about the last status of the hidden layer\n",
    "\n",
    "def build_model(tparams, options, Wemb):\n",
    "    trng = RandomStreams(123)\n",
    "    use_noise = theano.shared(numpy_floatX(0.))\n",
    "\n",
    "    x = T.matrix('x', dtype='int32')\n",
    "    mask = T.matrix('mask', dtype=config.floatX)\n",
    "    y = T.vector('y', dtype='int32')\n",
    "\n",
    "    n_timesteps = x.shape[0]\n",
    "    n_samples = x.shape[1]\n",
    "\n",
    "    emb = Wemb[x.flatten()].reshape([n_timesteps,n_samples,options['inputDimSize']])\n",
    "\n",
    "    proj = gru_layer(tparams, emb, options, mask=mask)\n",
    "    if options['use_dropout']: proj = dropout_layer(proj, use_noise, trng)\n",
    "\n",
    "    p_y_given_x = T.nnet.sigmoid(T.dot(proj, tparams['W_logistic']) + tparams['b_logistic'])\n",
    "    L = -(y * T.flatten(T.log(p_y_given_x)) + (1 - y) * T.flatten(T.log(1 - p_y_given_x)))\n",
    "    cost = T.mean(L)\n",
    "\n",
    "    if options['L2_reg'] > 0.: cost += options['L2_reg'] * (tparams['W_logistic'] ** 2).sum()\n",
    "\n",
    "    return use_noise, x, mask, y, p_y_given_x, cost\n",
    "\n",
    "# def load_data(seqFile, labelFile, timeFile=''):\n",
    "#     sequences = np.array(pickle.load(open(seqFile, 'rb')))\n",
    "#     labels = np.array(pickle.load(open(labelFile, 'rb')))\n",
    "#     if len(timeFile) > 0:\n",
    "#         times = np.array(pickle.load(open(timeFile, 'rb')))\n",
    "\n",
    "#     dataSize = len(labels)\n",
    "#     ind = np.random.permutation(dataSize)\n",
    "#     nTest = int(0.10 * dataSize)\n",
    "#     nValid = int(0.10 * dataSize)\n",
    "\n",
    "#     test_indices = ind[:nTest]\n",
    "#     valid_indices = ind[nTest:nTest+nValid]\n",
    "#     train_indices = ind[nTest+nValid:]\n",
    "\n",
    "#     train_set_x = sequences[train_indices]\n",
    "#     train_set_y = labels[train_indices]\n",
    "#     test_set_x = sequences[test_indices]\n",
    "#     test_set_y = labels[test_indices]\n",
    "#     valid_set_x = sequences[valid_indices]\n",
    "#     valid_set_y = labels[valid_indices]\n",
    "#     train_set_t = None\n",
    "#     test_set_t = None\n",
    "#     valid_set_t = None\n",
    "\n",
    "#     if len(timeFile) > 0:\n",
    "#         train_set_t = times[train_indices]\n",
    "#         test_set_t = times[test_indices]\n",
    "#         valid_set_t = times[valid_indices]\n",
    "\n",
    "#     def len_argsort(seq):\n",
    "#         return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "\n",
    "#     train_sorted_index = len_argsort(train_set_x)\n",
    "#     train_set_x = [train_set_x[i] for i in train_sorted_index]\n",
    "#     train_set_y = [train_set_y[i] for i in train_sorted_index]\n",
    "\n",
    "#     valid_sorted_index = len_argsort(valid_set_x)\n",
    "#     valid_set_x = [valid_set_x[i] for i in valid_sorted_index]\n",
    "#     valid_set_y = [valid_set_y[i] for i in valid_sorted_index]\n",
    "\n",
    "#     test_sorted_index = len_argsort(test_set_x)\n",
    "#     test_set_x = [test_set_x[i] for i in test_sorted_index]\n",
    "#     test_set_y = [test_set_y[i] for i in test_sorted_index]\n",
    "\n",
    "#     if len(timeFile) > 0:\n",
    "#         train_set_t = [train_set_t[i] for i in train_sorted_index]\n",
    "#         valid_set_t = [valid_set_t[i] for i in valid_sorted_index]\n",
    "#         test_set_t = [test_set_t[i] for i in test_sorted_index]\n",
    "\n",
    "#     train_set = (train_set_x, train_set_y, train_set_t)\n",
    "#     valid_set = (valid_set_x, valid_set_y, valid_set_t)\n",
    "#     test_set = (test_set_x, test_set_y, test_set_t)\n",
    "\n",
    "#     return train_set, valid_set, test_set\n",
    "\n",
    "def adadelta(tparams, grads, x, mask, y, cost):\n",
    "    zipped_grads = [theano.shared(p.get_value() * numpy_floatX(0.), name='%s_grad' % k) for k, p in tparams.iteritems()]\n",
    "    running_up2 = [theano.shared(p.get_value() * numpy_floatX(0.), name='%s_rup2' % k) for k, p in tparams.iteritems()]\n",
    "    running_grads2 = [theano.shared(p.get_value() * numpy_floatX(0.), name='%s_rgrad2' % k) for k, p in tparams.iteritems()]\n",
    "\n",
    "    zgup = [(zg, g) for zg, g in zip(zipped_grads, grads)]\n",
    "    rg2up = [(rg2, 0.95 * rg2 + 0.05 * (g ** 2)) for rg2, g in zip(running_grads2, grads)]\n",
    "\n",
    "    f_grad_shared = theano.function([x, mask, y], cost, updates=zgup + rg2up, name='adadelta_f_grad_shared')\n",
    "\n",
    "    updir = [-T.sqrt(ru2 + 1e-6) / T.sqrt(rg2 + 1e-6) * zg for zg, ru2, rg2 in zip(zipped_grads, running_up2, running_grads2)]\n",
    "    ru2up = [(ru2, 0.95 * ru2 + 0.05 * (ud ** 2)) for ru2, ud in zip(running_up2, updir)]\n",
    "    param_up = [(p, p + ud) for p, ud in zip(tparams.values(), updir)]\n",
    "\n",
    "    f_update = theano.function([], [], updates=ru2up + param_up, on_unused_input='ignore', name='adadelta_f_update')\n",
    "\n",
    "    return f_grad_shared, f_update\n",
    "\n",
    "# def calculate_auc(test_model, datasets):\n",
    "#     batchSize = 10\n",
    "#     n_batches = int(np.ceil(float(len(datasets[0])) / float(batchSize)))\n",
    "#     scoreVec = []\n",
    "#     for index in xrange(n_batches):\n",
    "#         x, mask = padMatrix(datasets[0][index*batchSize: (index+1)*batchSize])\n",
    "#         # FIX?:\n",
    "#         # ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.\n",
    "#         # scoreVec.extend(list(test_model(x, mask)))\n",
    "#         scoreVec.extend(test_model(x, mask))\n",
    "#     labels = datasets[1]\n",
    "# #     import numpy as np\n",
    "# #     from sklearn.metrics import roc_auc_score\n",
    "# #     y_true = np.array([0, 0, 1, 1])\n",
    "# #     y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "# #     roc_auc_score(y_true, y_scores)\n",
    "#     print('labels:', labels, 'scoreVec', list(scoreVec))\n",
    "#     auc = roc_auc_score(list(labels), list(scoreVec))\n",
    "#     return auc\n",
    "\n",
    "# def padMatrix(seqs):\n",
    "#     lengths = [len(s) for s in seqs]\n",
    "#     n_samples = len(seqs)\n",
    "#     maxlen = np.max(lengths)\n",
    "\n",
    "#     x = np.zeros((maxlen, n_samples)).astype('int32')\n",
    "#     x_mask = np.zeros((maxlen, n_samples)).astype(config.floatX)\n",
    "#     for idx, s in enumerate(seqs):\n",
    "# #         print (\"idx %s s: %s, x: %r, lens: %r, lengths[idx]: %s\" % (idx, s, x, lengths, lengths[idx]))\n",
    "#         x[:lengths[idx], idx] = s\n",
    "#         x_mask[:lengths[idx], idx] = 1.\n",
    "\n",
    "#     return x, x_mask\n",
    "\n",
    "def train_GRU_RNN(\n",
    "    seqFile='data.txt',\n",
    "    labelFile='label.txt',\n",
    "    outFile='out.txt',\n",
    "    inputDimSize= 100,\n",
    "    hiddenDimSize=100,\n",
    "    max_epochs=100,\n",
    "    L2_reg = 0.,\n",
    "    batchSize=100,\n",
    "    use_dropout=True,\n",
    "    useTime=False\n",
    "):\n",
    "    options = locals().copy()\n",
    "    \n",
    "    print 'Loading data ... ',\n",
    "    if simpleLoad:\n",
    "        trainSet, validSet, testSet = load_data_simple(seqFile, labelFile, '')\n",
    "    else:\n",
    "        trainSet, validSet, testSet = load_data(seqFile, labelFile, '')\n",
    "    # trainSet, validSet, testSet = load_data(seqFile, labelFile)\n",
    "    n_batches = int(np.ceil(float(len(trainSet[0])) / float(batchSize)))\n",
    "    print 'done!!'\n",
    "\n",
    "    print 'Building the model ... ',\n",
    "    params = init_params(options)\n",
    "    tparams = init_tparams(params)\n",
    "    Wemb = theano.shared(params['W_emb'], name='W_emb')\n",
    "    use_noise, x, mask, y, p_y_given_x, cost =  build_model(tparams, options, Wemb)\n",
    "    print 'done!!'\n",
    "    \n",
    "    print 'Constructing the optimizer ... ',\n",
    "    grads = T.grad(cost, wrt=tparams.values())\n",
    "    f_grad_shared, f_update = adadelta(tparams, grads, x, mask, y, cost)\n",
    "    print 'done!!'\n",
    "\n",
    "    test_model = theano.function(inputs=[x, mask], outputs=p_y_given_x, name='test_model')\n",
    "\n",
    "    bestValidAuc = 0.\n",
    "    bestTestAuc = 0.\n",
    "    iteration = 0\n",
    "    bestParams = OrderedDict()\n",
    "    print 'Optimization start !!'\n",
    "    for epoch in xrange(max_epochs):\n",
    "        for index in random.sample(range(n_batches), n_batches):\n",
    "            use_noise.set_value(1.)\n",
    "            # x, mask = padMatrix(trainSet[0][index*batchSize:(index+1)*batchSize])\n",
    "            \n",
    "            # e.g. \n",
    "            # batchX = dataset[0][index*batchSize:(index+1)*batchSize]\n",
    "            # ref: \n",
    "            # def padMatrix(seqs):\n",
    "            #     lengths = [len(s) for s in seqs]\n",
    "            #     n_samples = len(seqs)\n",
    "            #     maxlen = np.max(lengths)\n",
    "\n",
    "            #     x = np.zeros((maxlen, n_samples)).astype('int32')\n",
    "            #     x_mask = np.zeros((maxlen, n_samples)).astype(config.floatX)\n",
    "            #     for idx, s in enumerate(seqs):\n",
    "            # #         print (\"idx %s s: %s, x: %r, lens: %r, lengths[idx]: %s\" % (idx, s, x, lengths, lengths[idx]))\n",
    "            #         x[:lengths[idx], idx] = s\n",
    "            #         x_mask[:lengths[idx], idx] = 1.\n",
    "            #     return x, x_mask\n",
    "            \n",
    "            x, mask = padMatrixWithoutTime(trainSet[0][index*batchSize:(index+1)*batchSize], options)\n",
    "            y = trainSet[1][index*batchSize:(index+1)*batchSize]\n",
    "            cost = f_grad_shared(x, mask, y)\n",
    "            f_update()\n",
    "            iteration += 1\n",
    "\n",
    "        use_noise.set_value(0.)\n",
    "        validAuc = calculate_auc(test_model, validSet)\n",
    "        print 'epoch:%d, valid_auc:%f' % (epoch, validAuc)\n",
    "        if (validAuc > bestValidAuc):\n",
    "            bestValidAuc = validAuc\n",
    "            testAuc = calculate_auc(test_model, testSet)\n",
    "            bestTestAuc = testAuc\n",
    "            bestParams = unzip(tparams)\n",
    "            print 'Currenlty the best test_auc:%f' % testAuc\n",
    "    \n",
    "    np.savez_compressed(outFile, **bestParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...  done!!\n",
      "Building the model ...  done!!\n",
      "Constructing the optimizer ...  done!!\n",
      "Optimization start !!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Bad input argument to theano function with name \"adadelta_f_grad_shared\" at index 0 (0-based).  \nBacktrace when that variable is created:\n\n  File \"/usr/local/conda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/conda2/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/conda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/conda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2828, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/conda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-a077afbdae06>\", line 22, in <module>\n    use_dropout=use_dropout)\n  File \"<ipython-input-6-7332f667985b>\", line 213, in train_GRU_RNN\n    use_noise, x, mask, y, p_y_given_x, cost =  build_model(tparams, options, Wemb)\n  File \"<ipython-input-6-7332f667985b>\", line 58, in build_model\n    x = T.matrix('x', dtype='int32')\nTensorType(int32, matrix) cannot store a value of dtype float64 without risking loss of precision. If you do not mind this loss, you can: 1) explicitly cast your data to int32, or 2) set \"allow_input_downcast=True\" when calling \"function\". Value: \"array([[[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]]])\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a077afbdae06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mL2_reg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mL2_reg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mbatchSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     use_dropout=use_dropout)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-7332f667985b>\u001b[0m in \u001b[0;36mtrain_GRU_RNN\u001b[0;34m(seqFile, labelFile, outFile, inputDimSize, hiddenDimSize, max_epochs, L2_reg, batchSize, use_dropout, useTime)\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpadMatrixWithoutTime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainSet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainSet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_grad_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0mf_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/conda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    811\u001b[0m                         s.storage[0] = s.type.filter(\n\u001b[1;32m    812\u001b[0m                             \u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 813\u001b[0;31m                             allow_downcast=s.allow_downcast)\n\u001b[0m\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/conda2/lib/python2.7/site-packages/theano/tensor/type.pyc\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, data, strict, allow_downcast)\u001b[0m\n\u001b[1;32m    138\u001b[0m                             \u001b[0;34m'\"function\". Value: \"%s\"'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                             % (self, data.dtype, self.dtype, repr(data)))\n\u001b[0;32m--> 140\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                 elif (allow_downcast is None and\n\u001b[1;32m    142\u001b[0m                         \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Bad input argument to theano function with name \"adadelta_f_grad_shared\" at index 0 (0-based).  \nBacktrace when that variable is created:\n\n  File \"/usr/local/conda2/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/conda2/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/conda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/conda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2828, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/conda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-a077afbdae06>\", line 22, in <module>\n    use_dropout=use_dropout)\n  File \"<ipython-input-6-7332f667985b>\", line 213, in train_GRU_RNN\n    use_noise, x, mask, y, p_y_given_x, cost =  build_model(tparams, options, Wemb)\n  File \"<ipython-input-6-7332f667985b>\", line 58, in build_model\n    x = T.matrix('x', dtype='int32')\nTensorType(int32, matrix) cannot store a value of dtype float64 without risking loss of precision. If you do not mind this loss, you can: 1) explicitly cast your data to int32, or 2) set \"allow_input_downcast=True\" when calling \"function\". Value: \"array([[[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]]])\""
     ]
    }
   ],
   "source": [
    "gruOhFileBase = fileBase + \".gru_oh\"\n",
    "# The path to the output models. The models will be saved after every epoch\n",
    "outFile= gruOhFileBase + '.model'\n",
    "\n",
    "inputDimSize = 20000 #The number of unique medical codes\n",
    "hiddenDimSize = 100 #The size of the hidden layer of the GRU\n",
    "max_epochs = 30 #Maximum epochs to train\n",
    "L2_reg = 0.001 #L2 regularization for the logistic weight\n",
    "batchSize = 10 #The size of the mini-batch\n",
    "use_dropout = True #Whether to use a dropout between the GRU and the logistic layer\n",
    "useTime = False\n",
    "\n",
    "train_GRU_RNN(\n",
    "    seqFile=seqFile, \n",
    "    labelFile=labelFile, \n",
    "    outFile=outFile, \n",
    "    inputDimSize=inputDimSize, \n",
    "    hiddenDimSize=hiddenDimSize, \n",
    "    max_epochs=max_epochs, \n",
    "    L2_reg=L2_reg, \n",
    "    batchSize=batchSize, \n",
    "    use_dropout=use_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU OneHot Time\n",
    "\n",
    "```\n",
    "python gru_onehot_time.py sequences.pkl times.pkl labels.pkl\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
